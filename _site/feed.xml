<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-12-08T10:31:24-05:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Your awesome title</title><subtitle>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.</subtitle><entry><title type="html">Welcome to Jekyll!</title><link href="http://localhost:4000/jekyll/update/2023/11/30/welcome-to-jekyll.html" rel="alternate" type="text/html" title="Welcome to Jekyll!" /><published>2023-11-30T13:41:00-05:00</published><updated>2023-11-30T13:41:00-05:00</updated><id>http://localhost:4000/jekyll/update/2023/11/30/welcome-to-jekyll</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2023/11/30/welcome-to-jekyll.html"><![CDATA[<p>You’ll find this post in your <code class="language-plaintext highlighter-rouge">_posts</code> directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run <code class="language-plaintext highlighter-rouge">jekyll serve</code>, which launches a web server and auto-regenerates your site when a file is updated.</p>

<p>Jekyll requires blog post files to be named according to the following format:</p>

<p><code class="language-plaintext highlighter-rouge">YEAR-MONTH-DAY-title.MARKUP</code></p>

<p>Where <code class="language-plaintext highlighter-rouge">YEAR</code> is a four-digit number, <code class="language-plaintext highlighter-rouge">MONTH</code> and <code class="language-plaintext highlighter-rouge">DAY</code> are both two-digit numbers, and <code class="language-plaintext highlighter-rouge">MARKUP</code> is the file extension representing the format used in the file. After that, include the necessary front matter. Take a look at the source for this post to get an idea about how it works.</p>

<p>Jekyll also offers powerful support for code snippets:</p>

<figure class="highlight"><pre><code class="language-ruby" data-lang="ruby"><span class="k">def</span> <span class="nf">print_hi</span><span class="p">(</span><span class="nb">name</span><span class="p">)</span>
  <span class="nb">puts</span> <span class="s2">"Hi, </span><span class="si">#{</span><span class="nb">name</span><span class="si">}</span><span class="s2">"</span>
<span class="k">end</span>
<span class="n">print_hi</span><span class="p">(</span><span class="s1">'Tom'</span><span class="p">)</span>
<span class="c1">#=&gt; prints 'Hi, Tom' to STDOUT.</span></code></pre></figure>

<p>Check out the <a href="https://jekyllrb.com/docs/home">Jekyll docs</a> for more info on how to get the most out of Jekyll. File all bugs/feature requests at <a href="https://github.com/jekyll/jekyll">Jekyll’s GitHub repo</a>. If you have questions, you can ask them on <a href="https://talk.jekyllrb.com/">Jekyll Talk</a>.</p>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[You’ll find this post in your _posts directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run jekyll serve, which launches a web server and auto-regenerates your site when a file is updated.]]></summary></entry><entry><title type="html">Week 2: 3D Scanning Workflows</title><link href="http://localhost:4000/research/2023/11/01/week-2-3D-Scanning-Workflows.html" rel="alternate" type="text/html" title="Week 2: 3D Scanning Workflows" /><published>2023-11-01T18:21:59-04:00</published><updated>2023-11-01T18:21:59-04:00</updated><id>http://localhost:4000/research/2023/11/01/week-2-3D-Scanning-Workflows</id><content type="html" xml:base="http://localhost:4000/research/2023/11/01/week-2-3D-Scanning-Workflows.html"><![CDATA[<p>On October 24th I met with John to go over the 3D scanning and 3D printing workflows. <img src="tn/images/5.jpg" alt="drawing" width="200" /> 
We compared: Photogrammetry vs Neural Radiance Fields Vs 3D gaussian splats. <img src="tn/images/artex.jpg" alt="drawing" width="200" /></p>

<blockquote>
  <p>
  
  </p>
</blockquote>]]></content><author><name>D Pillis</name></author><category term="research" /><summary type="html"><![CDATA[On October 24th I met with John to go over the 3D scanning and 3D printing workflows. We compared: Photogrammetry vs Neural Radiance Fields Vs 3D gaussian splats.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/images/9.jpg" /><media:content medium="image" url="http://localhost:4000/images/9.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Week 3. Contemporary Techniques</title><link href="http://localhost:4000/research/2023/11/01/week-3-contermporary-techniques.html" rel="alternate" type="text/html" title="Week 3. Contemporary Techniques" /><published>2023-11-01T18:21:59-04:00</published><updated>2023-11-01T18:21:59-04:00</updated><id>http://localhost:4000/research/2023/11/01/week-3-contermporary-techniques</id><content type="html" xml:base="http://localhost:4000/research/2023/11/01/week-3-contermporary-techniques.html"><![CDATA[<p>On October 25th I met with Quentin and we looked at workflows for planning how to make a 3D scanner. We thought of this as a portable lightstage or portable panoptic studio for small objects. 20-30 Rasperry Pi Cameras. Initial plans: Rasperry Pi Zero with Camera Cable-3D Printed Geodesc Dome of Cameras-Expandable Bucky Ball- Compute with SIFT for each image. Stream the data super fast.Goal- Instant 3D object scanning- Top and bottom. Explore contemporary formats like ThreeJS and MeshGPT. I experimented with Polycam’s floorplan tool and it is disappointing. It renders objects as if they are in a domestic setting automatically. It didn’t work for me when I tried to scan an unconventional space. I used grabCad to find models of seeed studio xiao nrf52840 3d model.</p>
<blockquote>
  <p>
  
  </p>
</blockquote>]]></content><author><name>D Pillis</name></author><category term="research" /><summary type="html"><![CDATA[On October 25th I met with Quentin and we looked at workflows for planning how to make a 3D scanner. We thought of this as a portable lightstage or portable panoptic studio for small objects. 20-30 Rasperry Pi Cameras. Initial plans: Rasperry Pi Zero with Camera Cable-3D Printed Geodesc Dome of Cameras-Expandable Bucky Ball- Compute with SIFT for each image. Stream the data super fast.Goal- Instant 3D object scanning- Top and bottom. Explore contemporary formats like ThreeJS and MeshGPT. I experimented with Polycam’s floorplan tool and it is disappointing. It renders objects as if they are in a domestic setting automatically. It didn’t work for me when I tried to scan an unconventional space. I used grabCad to find models of seeed studio xiao nrf52840 3d model.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/images/8.jpg" /><media:content medium="image" url="http://localhost:4000/images/8.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Week 4: 3D Software</title><link href="http://localhost:4000/research/2023/11/01/week-4-3d-software.html" rel="alternate" type="text/html" title="Week 4: 3D Software" /><published>2023-11-01T18:21:59-04:00</published><updated>2023-11-01T18:21:59-04:00</updated><id>http://localhost:4000/research/2023/11/01/week-4-3d-software</id><content type="html" xml:base="http://localhost:4000/research/2023/11/01/week-4-3d-software.html"><![CDATA[<p>On October 31st I manufactured six holokits on the laser cutter to use for a tutorial on mixed reality. We wanted the students to be able to make their own holographic displays. <img src="tn/images/holokit.jpg" alt="drawing" width="200" /></p>

<blockquote>
  <p>
    This research explores a new approach to tracking hands, or any articulated model, by using an augmented rigid body simulation. This allows us to phrase 3D object tracking as a linear complementarity problem with a well-defined solution. Based on a depth sensor&#8217;s samples, the system generates constraints that limit motion orthogonal to the rigid body model&#8217;s surface. These constraints, along with prior motion, collision/contact constraints, and joint mechanics, are resolved with a projected Gauss-Seidel solver. Due to camera noise properties and attachment errors, the numerous surface constraints are impulse capped to avoid overpowering mechanical constraints. To improve tracking accuracy, multiple simulations are spawned at each frame and fed a variety of heuristics, constraints and poses. A 3D error metric selects the best-fit simulation, helping the system handle challenging hand motions.
  </p>
</blockquote>]]></content><author><name>D Pillis</name></author><category term="research" /><summary type="html"><![CDATA[On October 31st I manufactured six holokits on the laser cutter to use for a tutorial on mixed reality. We wanted the students to be able to make their own holographic displays.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/images/6.jpg" /><media:content medium="image" url="http://localhost:4000/images/6.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Week 5. 3D Hardware</title><link href="http://localhost:4000/research/2023/11/01/week-5-3D-hardware.html" rel="alternate" type="text/html" title="Week 5. 3D Hardware" /><published>2023-11-01T18:21:59-04:00</published><updated>2023-11-01T18:21:59-04:00</updated><id>http://localhost:4000/research/2023/11/01/week-5-3D-hardware</id><content type="html" xml:base="http://localhost:4000/research/2023/11/01/week-5-3D-hardware.html"><![CDATA[<p>In early November, I ordered 6 XIAO ESP32S3 cameras. I met with Quentin in CBA to discuss his previous projects exploring custom 3D scanners. His made use of an arm that moved and rotated around an object. We decided that imitating a light stage design would be better, so we devised a plan to use small embedded camera sensors to capture multiple images. We decided to order the following: https://www.seeedstudio.com/OV5640-Camera-for-XIAO-ESP32S3-Sense-With-Heat-Sink-p-5739.html. I started to explore how this might work by implementing the gaussian splatting algorithim locally on my machine so that I could run simple 3D scan processes. Based on these experiements, we came up with a plan to construct a cage or structure rot of in the design of a buckminster fuller ball to capture objects. This specficially applies to my thesis because I need to scan a massive amount of small objects. I describe this plan in an excerpt of my thesis proposal below:
<img src="tn/images/cage-test.png" alt="drawing" width="200" /></p>
<blockquote>
  <p>
In an independent study with Professor Neil Gershenfeld, I am learning how to design custom 3D scanning interfaces to use for personal object archiving. This requires multi-camera capture to sequence multiple images together, resulting in low poly but high resolution 3D models (gaussian splats) of objects. These hardware tools will provide a tangible toolkit for the process of life-logging large archives of personal possessions for use in the simulation environment. In the independent study, I am also developing pipelines to which draw inspiration from Meta’s Codec Avatar pipeline for virtually indistinguishable digital doubles. I am specifically applying this to the relationship between an individual and their environment. This process is connected to the processes explored in the 3D scanning workflows, but evolved to include a proposal for a computational model. The goal is to implement this in the phone booth.
  </p>
</blockquote>]]></content><author><name>D Pillis</name></author><category term="research" /><summary type="html"><![CDATA[In early November, I ordered 6 XIAO ESP32S3 cameras. I met with Quentin in CBA to discuss his previous projects exploring custom 3D scanners. His made use of an arm that moved and rotated around an object. We decided that imitating a light stage design would be better, so we devised a plan to use small embedded camera sensors to capture multiple images. We decided to order the following: https://www.seeedstudio.com/OV5640-Camera-for-XIAO-ESP32S3-Sense-With-Heat-Sink-p-5739.html. I started to explore how this might work by implementing the gaussian splatting algorithim locally on my machine so that I could run simple 3D scan processes. Based on these experiements, we came up with a plan to construct a cage or structure rot of in the design of a buckminster fuller ball to capture objects. This specficially applies to my thesis because I need to scan a massive amount of small objects. I describe this plan in an excerpt of my thesis proposal below: In an independent study with Professor Neil Gershenfeld, I am learning how to design custom 3D scanning interfaces to use for personal object archiving. This requires multi-camera capture to sequence multiple images together, resulting in low poly but high resolution 3D models (gaussian splats) of objects. These hardware tools will provide a tangible toolkit for the process of life-logging large archives of personal possessions for use in the simulation environment. In the independent study, I am also developing pipelines to which draw inspiration from Meta’s Codec Avatar pipeline for virtually indistinguishable digital doubles. I am specifically applying this to the relationship between an individual and their environment. This process is connected to the processes explored in the 3D scanning workflows, but evolved to include a proposal for a computational model. The goal is to implement this in the phone booth.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/images/7.jpg" /><media:content medium="image" url="http://localhost:4000/images/7.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Week 1-Principles and practices</title><link href="http://localhost:4000/research/2023/11/01/week-1-Principles-and-practices.html" rel="alternate" type="text/html" title="Week 1-Principles and practices" /><published>2023-11-01T18:21:59-04:00</published><updated>2023-11-01T18:21:59-04:00</updated><id>http://localhost:4000/research/2023/11/01/week-1-Principles-and-practices</id><content type="html" xml:base="http://localhost:4000/research/2023/11/01/week-1-Principles-and-practices.html"><![CDATA[<p>On October 19th, for week 1 of the independent study, I met with Dr. Gershenfeld to discuss plans for using the 3D scanners in the E14 workspace of the Center for Bits and Atoms.</p>
<blockquote>
  <p>
  We proposed that I would make holokit prototypes as part of the initial research project into imaging technology. I started by learning how to use the Bambuu 3D printer. Bambuu is state of the art 3D printer that John set me up with in the HTMAA space. I focused my first few weeks on learning now to use the Artec Leo for high quality color 3D scanning. I plan collaborate with shop staff to use the Stratasys J850 to 3D print objects that will be duplicates, inversions, miniatures and variations of the scanned objects.
  </p>
</blockquote>]]></content><author><name>D Pillis</name></author><category term="research" /><summary type="html"><![CDATA[On October 19th, for week 1 of the independent study, I met with Dr. Gershenfeld to discuss plans for using the 3D scanners in the E14 workspace of the Center for Bits and Atoms. We proposed that I would make holokit prototypes as part of the initial research project into imaging technology. I started by learning how to use the Bambuu 3D printer. Bambuu is state of the art 3D printer that John set me up with in the HTMAA space. I focused my first few weeks on learning now to use the Artec Leo for high quality color 3D scanning. I plan collaborate with shop staff to use the Stratasys J850 to 3D print objects that will be duplicates, inversions, miniatures and variations of the scanned objects.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/images/10.jpg" /><media:content medium="image" url="http://localhost:4000/images/10.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Week 7: 3D Printer Types</title><link href="http://localhost:4000/research/2023/11/01/week-7-3D-Printer-Types.html" rel="alternate" type="text/html" title="Week 7: 3D Printer Types" /><published>2023-11-01T18:21:59-04:00</published><updated>2023-11-01T18:21:59-04:00</updated><id>http://localhost:4000/research/2023/11/01/week-7-3D%20Printer%20Types</id><content type="html" xml:base="http://localhost:4000/research/2023/11/01/week-7-3D-Printer-Types.html"><![CDATA[<p>This week I learned about different types of 3D printers. I worked with Wedyan and Yun to learn how to use the Form Labs printers that we have in the lab and in CBA. Using the I made some new 3D prints of a mechanical robot arm and some sample prints of miniatures from the Tangible Media group lab. My next idea is to use the Stratsys printer to prototype miniatures from the tangible media lab that can actuate. I hope to make a miniature version of Transform and InTouch as part of my final project. The draft version is seen below: <img src="tn/images/transform.jpg" alt="drawing" width="200" /> <img src="tn/images/intouch.JPG" alt="drawing" width="200" /> <img src="tn/images/print4" alt="drawing" width="200" /></p>
<blockquote>
  <p>
    I talked with Ozgun and she suggested we use a rack and pin rotation mechanism inside the print so that we could indiviually move the top pins along some general motion. This seems like an accessible way to make a toy transform that would demonstrate its appeal and fundamental function.
  </p>
</blockquote>]]></content><author><name>D Pillis</name></author><category term="research" /><summary type="html"><![CDATA[This week I learned about different types of 3D printers. I worked with Wedyan and Yun to learn how to use the Form Labs printers that we have in the lab and in CBA. Using the I made some new 3D prints of a mechanical robot arm and some sample prints of miniatures from the Tangible Media group lab. My next idea is to use the Stratsys printer to prototype miniatures from the tangible media lab that can actuate. I hope to make a miniature version of Transform and InTouch as part of my final project. The draft version is seen below: I talked with Ozgun and she suggested we use a rack and pin rotation mechanism inside the print so that we could indiviually move the top pins along some general motion. This seems like an accessible way to make a toy transform that would demonstrate its appeal and fundamental function.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/images/4.jpg" /><media:content medium="image" url="http://localhost:4000/images/4.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Week 8: 3D Printer Prototypes</title><link href="http://localhost:4000/research/2023/11/01/week-8-3D-printer-prototypes.html" rel="alternate" type="text/html" title="Week 8: 3D Printer Prototypes" /><published>2023-11-01T18:21:59-04:00</published><updated>2023-11-01T18:21:59-04:00</updated><id>http://localhost:4000/research/2023/11/01/week-8-3D-printer-prototypes</id><content type="html" xml:base="http://localhost:4000/research/2023/11/01/week-8-3D-printer-prototypes.html"><![CDATA[<p>This week I learned about different types of 3D printer prototypes and how to make machines, since students in HTMAA had ‘machine week’.</p>
<blockquote>
  <p>
    This research explores a new approach to tracking hands, or any articulated model, by using an augmented rigid body simulation. This allows us to phrase 3D object tracking as a linear complementarity problem with a well-defined solution. Based on a depth sensor&#8217;s samples, the system generates constraints that limit motion orthogonal to the rigid body model&#8217;s surface. These constraints, along with prior motion, collision/contact constraints, and joint mechanics, are resolved with a projected Gauss-Seidel solver. Due to camera noise properties and attachment errors, the numerous surface constraints are impulse capped to avoid overpowering mechanical constraints. To improve tracking accuracy, multiple simulations are spawned at each frame and fed a variety of heuristics, constraints and poses. A 3D error metric selects the best-fit simulation, helping the system handle challenging hand motions.
  </p>
</blockquote>]]></content><author><name>D Pillis</name></author><category term="research" /><summary type="html"><![CDATA[This week I learned about different types of 3D printer prototypes and how to make machines, since students in HTMAA had ‘machine week’. This research explores a new approach to tracking hands, or any articulated model, by using an augmented rigid body simulation. This allows us to phrase 3D object tracking as a linear complementarity problem with a well-defined solution. Based on a depth sensor&#8217;s samples, the system generates constraints that limit motion orthogonal to the rigid body model&#8217;s surface. These constraints, along with prior motion, collision/contact constraints, and joint mechanics, are resolved with a projected Gauss-Seidel solver. Due to camera noise properties and attachment errors, the numerous surface constraints are impulse capped to avoid overpowering mechanical constraints. To improve tracking accuracy, multiple simulations are spawned at each frame and fed a variety of heuristics, constraints and poses. A 3D error metric selects the best-fit simulation, helping the system handle challenging hand motions.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/images/3.jpg" /><media:content medium="image" url="http://localhost:4000/images/3.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Week 9. 3D Scanner Types</title><link href="http://localhost:4000/research/2023/11/01/week-9-3d-scanner-types.html" rel="alternate" type="text/html" title="Week 9. 3D Scanner Types" /><published>2023-11-01T18:21:59-04:00</published><updated>2023-11-01T18:21:59-04:00</updated><id>http://localhost:4000/research/2023/11/01/week-9-3d-scanner-types</id><content type="html" xml:base="http://localhost:4000/research/2023/11/01/week-9-3d-scanner-types.html"><![CDATA[<p>I experimented with three different approaches to scanning this week. Using the Artec Leo, the 3D sense, and the iPhone Lidar. Each of them produces a different quality scan. Some of these scans can be seen below. <img src="tn/images/tmg-scans.png" alt="drawing" width="200" /> <img src="tn/images/room-combusted.png" alt="drawing" width="200" /> <img src="tn/images/tmg-scans.png" alt="drawing" width="200" /></p>

<blockquote>
  <p>
    This research explores a new approach to tracking hands, or any articulated model, by using an augmented rigid body simulation. This allows us to phrase 3D object tracking as a linear complementarity problem with a well-defined solution. Based on a depth sensor&#8217;s samples, the system generates constraints that limit motion orthogonal to the rigid body model&#8217;s surface. These constraints, along with prior motion, collision/contact constraints, and joint mechanics, are resolved with a projected Gauss-Seidel solver. Due to camera noise properties and attachment errors, the numerous surface constraints are impulse capped to avoid overpowering mechanical constraints. To improve tracking accuracy, multiple simulations are spawned at each frame and fed a variety of heuristics, constraints and poses. A 3D error metric selects the best-fit simulation, helping the system handle challenging hand motions.
  </p>
</blockquote>]]></content><author><name>D Pillis</name></author><category term="research" /><summary type="html"><![CDATA[I experimented with three different approaches to scanning this week. Using the Artec Leo, the 3D sense, and the iPhone Lidar. Each of them produces a different quality scan. Some of these scans can be seen below.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/images/2.jpg" /><media:content medium="image" url="http://localhost:4000/images/2.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Week 10: 3D Scanner Prototype</title><link href="http://localhost:4000/research/2023/11/01/week-10-3d-scanner-prototype.html" rel="alternate" type="text/html" title="Week 10: 3D Scanner Prototype" /><published>2023-11-01T18:21:59-04:00</published><updated>2023-11-01T18:21:59-04:00</updated><id>http://localhost:4000/research/2023/11/01/week-10-3d-scanner-prototype</id><content type="html" xml:base="http://localhost:4000/research/2023/11/01/week-10-3d-scanner-prototype.html"><![CDATA[<p>The final project of this independent study is culminating in a custom 3D scanner. This will be finished on at the end of the semester. I soent about a week troubleshooting the XIAO ESP32 Sense before I finally undersood how to make them work. They need to be booted each time they are loaded, and after the script is run, they then need to be reset. The buttons are so small it is hard to see what to do. Once I figured out this workflow, it was much easier. I can now get a webserver running and had all 6 of the cameras working. The next issue is getting them to work at once. Quentin advised me to adjust the code so that the header is changed which will allow me to control the input of the camera.</p>
<blockquote>
  <p>
  </p>
</blockquote>]]></content><author><name>D Pillis</name></author><category term="research" /><summary type="html"><![CDATA[The final project of this independent study is culminating in a custom 3D scanner. This will be finished on at the end of the semester. I soent about a week troubleshooting the XIAO ESP32 Sense before I finally undersood how to make them work. They need to be booted each time they are loaded, and after the script is run, they then need to be reset. The buttons are so small it is hard to see what to do. Once I figured out this workflow, it was much easier. I can now get a webserver running and had all 6 of the cameras working. The next issue is getting them to work at once. Quentin advised me to adjust the code so that the header is changed which will allow me to control the input of the camera.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/images/1.jpg" /><media:content medium="image" url="http://localhost:4000/images/1.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>